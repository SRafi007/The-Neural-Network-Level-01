# Example of Email Classification

Let's consider a record of an email dataset:

| Email ID | Email Content | Sender | Subject Line | Label |
|----------|---------------|--------|--------------|-------|
| 1 | "Get free gift cards now!" | spam@example.com | "Exclusive Offer" | 1 |

To classify this email, we will create a feature vector based on the analysis of keywords such as "free," "win," and "offer."

The feature vector of the record can be presented as:
* "free": Present (1)
* "win": Absent (0)
* "offer": Present (1)

| Email ID | Email Content | Sender | Subject Line | Feature Vector | Label |
|----------|---------------|--------|--------------|----------------|-------|
| 1 | "Get free gift cards now!" | spam@example.com | "Exclusive Offer" | [1, 0, 1] | 1 |

Now, let's delve into the working:

## 1. Input Layer
The input layer contains 3 nodes that indicates the presence of each keyword.

## 2. Hidden Layer
* The input data is passed through one or more hidden layers.
* Each neuron in the hidden layer performs the following operations:
   1. **Weighted Sum**: Each input is multiplied by a corresponding weight assigned to the connection. For example, if the weights from the input layer to the hidden layer neurons are as follows:
      * Weights for Neuron H1: `[0.5, -0.2, 0.3]`
      * Weights for Neuron H2: `[0.4, 0.1, -0.5]`
   2. **Calculate Weighted Input**:
      * For Neuron H1:
         * Calculation = (1 × 0.5) + (0 × −0.2) + (1 × 0.3) = 0.5 + 0 + 0.3 = 0.8
      * For Neuron H2:
         * Calculation = (1 × 0.4) + (0 × 0.1) + (1 × −0.5) = 0.4 + 0 − 0.5 = −0.1
   3. **Activation Function**: The result is passed through an activation function (e.g., ReLU or sigmoid) to introduce non-linearity.
      * For H1, applying ReLU: ReLU(0.8) = 0.8
      * For H2, applying ReLU: ReLU(−0.1) = 0

## 3. Output Layer
* The activated outputs from the hidden layer are passed to the output neuron.
* The output neuron receives the values from the hidden layer neurons and computes the final prediction using weights:
   * Suppose the output weights from hidden layer to output neuron are `[0.7, 0.2]`.
   * Calculation:
      * Input = (0.8 × 0.7) + (0 × 0.2) = 0.56 + 0 = 0.56
   * **Final Activation**: The output is passed through a sigmoid activation function to obtain a probability:
      * σ(0.56) ≈ 0.636

## 4. Final Classification
* The output value of approximately **0.636** indicates the probability of the email being spam.
* Since this value is greater than 0.5, the neural network classifies the email as spam (1).

```mermaid
graph LR
    subgraph Input Layer
        I1[free: 1]
        I2[win: 0]
        I3[offer: 1]
    end

    subgraph Hidden Layer
        H1[H1: 0.8]
        H2[H2: 0]
    end

    subgraph Output Layer
        O1[Output: 0.636]
    end

    I1 -->|0.5| H1
    I2 -->|-0.2| H1
    I3 -->|0.3| H1
    I1 -->|0.4| H2
    I2 -->|0.1| H2
    I3 -->|-0.5| H2

    H1 -->|0.7| O1
    H2 -->|0.2| O1

    style I1 fill:#f9f,stroke:#333,stroke-width:2px
    style I2 fill:#f9f,stroke:#333,stroke-width:2px
    style I3 fill:#f9f,stroke:#333,stroke-width:2px
    style H1 fill:#bbf,stroke:#333,stroke-width:2px
    style H2 fill:#bbf,stroke:#333,stroke-width:2px
    style O1 fill:#bfb,stroke:#333,stroke-width:2px
